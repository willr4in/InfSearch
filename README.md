# Гибридная Поисковая Система на C++ и Python

Этот проект представляет собой полноценную поисковую систему для текстовых документов, построенную на гибридной архитектуре:
- **C++ ядро (`core_cpp/`)**: Высокопроизводительный "движок", написанный на C++ без использования STL (за исключением токенизатора). Он отвечает за токенизацию, стемминг, создание инвертированного индекса, булев поиск и анализ по закону Ципфа.
- **Python-обвязка**: Набор скриптов на Python, которые управляют C++ ядром и предоставляют пользовательские интерфейсы (веб-сервер Flask, утилиты командной строки).

## Архитектура

- **Сбор данных (`crawler/`)**: Python-скрипт для скачивания статей из Википедии и сохранения их в MongoDB.
- **Обработка текста (`core_cpp/`)**: C++ библиотека `libcore.so` предоставляет функции для токенизации и стемминга (русский алгоритм Портера).
- **Индексация (`core_cpp/`, `search/`)**: C++ ядро строит инвертированный индекс на основе самописной хэш-таблицы и сохраняет его в бинарный файл (`boolean_index.bin`).
- **Поиск (`core_cpp/`, `search/`)**: C++ ядро загружает индекс и выполняет булевы запросы (`AND`, `OR`, `NOT`).
- **Анализ (`core_cpp/`, `analysis/`)**: C++ ядро рассчитывает частоты слов для анализа по закону Ципфа.
- **Интерфейсы**:
    - **Веб-сервер (`web/`)**: Приложение на Flask для поиска по индексу.
    - **Утилиты командной строки**: Скрипты для запуска индексации, токенизации и т.д.

## Установка и запуск

### 1. Требования

- Python 3.8+
- C++ компилятор (GCC/G++)
- CMake (версия 3.10+)
- MongoDB

### 2. Установка зависимостей

```bash
# Установка Python-библиотек
pip install -r crawler/requirements.txt
pip install -r web/requirements.txt
pip install -r requirements-dev.txt
```

### 3. Сборка C++ ядра

Необходимо скомпилировать C++ код в динамическую библиотеку.

```bash
# Создать директорию для сборки
cmake -S core_cpp -B core_cpp/build

# Собрать библиотеку
cmake --build core_cpp/build
```
В результате в директории `core_cpp/build/` появится файл `libcore.so`.

### 4. Полный цикл работы

**Шаг 1: Скачивание корпуса (если необходимо)**
```bash
python3 crawler/crawler.py
```

**Шаг 2: Токенизация и стемминг (использует C++ ядро)**
Этот скрипт обработает все документы в MongoDB.
```bash
python3 tokenizer/tokenize_batch.py
```

**Шаг 3: Построение бинарного индекса (использует C++ ядро)**
Этот скрипт создаст файл `boolean_index.bin` в корне проекта.
```bash
python3 search/build_boolean_index.py
```

**Шаг 4: Запуск поиска**

*   **Через веб-интерфейс:**
    ```bash
    python3 web/app.py
    ```
    Откройте в браузере `http://127.0.0.1:5000`.

*   **Через утилиту командной строки:**
    ```bash
    python3 search/boolean_search.py
    ```

### 5. Запуск тестов

Для проверки корректности работы C++ ядра можно запустить интеграционные тесты:
```bash
python3 -m pytest
```
